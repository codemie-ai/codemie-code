# Codex configuration with multiple providers
model = "gpt-4o"
approval_policy = "on-request"
sandbox_mode = "workspace-write"
model_reasoning_effort = "high"

[model_providers.openai-chat-completions]
name = "OpenAI using Chat Completions"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
wire_api = "chat"

[model_providers.mistral]
name = "Mistral"
base_url = "https://api.mistral.ai/v1"
env_key = "MISTRAL_API_KEY"

[profiles.openai]
model_provider = "openai-chat-completions"
model = "gpt-4o"

[profiles.deep-review]
model_provider = "mistral"
model = "mistral-large-latest"
model_reasoning_effort = "high"
approval_policy = "never"

[profiles.lightweight]
model_provider = "openai-chat-completions"
model = "gpt-4o-mini"
approval_policy = "untrusted"

[features]
streamable_shell = true
web_search_request = true

[shell_environment_policy]
include_only = ["PATH", "HOME"]

[projects."/Users/dev/webapp"]
trust_level = "trusted"

[projects."/Users/dev/scripts"]
trust_level = "untrusted"

# --- CODEMIE SESSION START: ollama-{timestamp} ---
profile = "ollama"

[model_providers.ollama]
name = "ollama"
base_url = "http://localhost:11434/v1"

[profiles.ollama]
model_provider = "ollama"
model = "qwen3-vl:235b-cloud"
# --- CODEMIE SESSION END: ollama-{timestamp} ---

